# -*- coding: utf-8 -*-
"""Full_pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SEY90dL7w5JSSeGcr5YgPg5IzLJqmKau
"""

#%%

!git clone https://github.com/quentinf00/my_ocb.git
!git clone https://github.com/CIA-Oceanix/4dvarnet-starter.git


#%%
# Commented out IPython magic to ensure Python compatibility.
%cd my_ocb/
!git fetch origin pipelining
!git checkout pipelining
!git stash
!git pull origin pipelining
%cd ..

# Commented out IPython magic to ensure Python compatibility.
%cd 4dvarnet-starter/
!git fetch origin ocb
!git checkout ocb
!git pull origin ocb
%cd ..

#%%
!mamba env create -q -f my_ocb/env.yaml -n ocb
!mamba env create -q -f 4dvarnet-starter/environment.yaml -n starter

#%%
!pip install -q -e my_ocb/modules/qf_interp_grid_on_track
!pip install -q -e my_ocb/modules/dz_download_ssh_tracks
!pip install -q -e my_ocb/modules/qf_filter_merge_daily_ssh_tracks
!pip install -q -e my_ocb/modules/alongtrack_lambdax
!pip install -q -e my_ocb/modules/dz_alongtrack_mu
!pip install -q -e my_ocb/modules/qf_simple_chaining
!pip install -q --no-deps -e my_ocb/pipelines/qf_alongtrack_metrics_from_map
!pip install -q --no-deps -e my_ocb/pipelines/qf_download_altimetry_constellation

!pip install -q -e 4dvarnet-starter/contrib/dz_4dvarnet_predict

#%%
!wget -nc https://s3.eu-central-1.wasabisys.com/melody/quentin_cloud/starter_jobs/new_baseline/.hydra/config.yaml
!wget -nc https://s3.eu-central-1.wasabisys.com/melody/quentin_cloud/starter_jobs/new_baseline/base/checkpoints/val_mse=3.01245-epoch=551.ckpt
!wget -nc https://s3.eu-central-1.wasabisys.com/melody/quentin_cloud/starter_jobs/new_baseline/.hydra/hydra.yaml

"""## Download inference data"""
#%%
!qf_download_altimetry_constellation --cfg job
#%%
!copernicusmarine login
#%%
!qf_download_altimetry_constellation -m 'to_run=[_01_fetch_inference_tracks]' stages._01_fetch_inference_tracks.sat=alg,h2ag,j2g,j2n,j3,s3a


#%%
!qf_download_altimetry_constellation -m 'to_run=[_02_filter_tracks]' stages._01_fetch_inference_tracks.sat=alg,h2ag,j2g,j2n,j3,s3a

#%%
!qf_download_altimetry_constellation 'to_run=[_03_merge_tracks]'

#%%
import xarray as xr
ds = xr.open_dataset('data/prepared/inference_combined.nc')
ds.close()
ds

#%%"""## Implement Gridding module"""
!python grid_chain.py --cfg job

!python grid_chain.py
#%%
import xarray as xr
ds = xr.open_dataset('data/prepared/gridded.nc')
ds.close()
ds.isel(time=slice(None, 4)).ssh.plot(col='time', col_wrap=2)

"""## 4dvarnet inference"""
#%%
!python my_predict.py --cfg job
#%%
!python my_predict.py \
    input_path=data/prepared/gridded.nc\
     output_dir=data/inferred_batches \
        batch_size=2
#%%

"""## Implement Batch merging module"""
#%%
from pathlib import Path
import numpy as np
import xarray as xr
import tqdm 
def triang(n, min=0.05):
    return np.clip(1 - np.abs(np.linspace(-1, 1, n)), min, 1.)

def hanning(n):
    import scipy
    return scipy.signal.windows.hann(n)

def bell(n, nstd=5):
    import scipy
    return scipy.signal.windows.gaussian(n, std=n/nstd)

def crop(n, crop=20):
    w = np.zeros(n)
    w[crop:-crop] = 1.
    return w

def build_weight(patch_dims, dim_weights=dict(time=triang, lat=crop, lon=crop)):
    return (
        dim_weights.get('time', np.ones)(patch_dims['time'])[:, None, None]
        * dim_weights.get('lat', np.ones)(patch_dims['lat'])[None, :, None]
        * dim_weights.get('lon', np.ones)(patch_dims['lon'])[None, None, :]
    )

def outer_add_das(das):
    out_coords = xr.merge([da.coords.to_dataset() for da in das])
    # print(f'{out_coords=}')
    fmt_das = [da.reindex_like(out_coords, fill_value=0.) for da in das]
    # print(fmt_das[0].shape)
    return sum(fmt_das)

def merge_batches(
    input_directory='data/inferred_batches',
    output_path='method_outputs/merged_batches.nc',
    weight=build_weight(patch_dims=dict(time=15, lat=240, lon=240)),
    batches_per_iter=10,
    ):
    batches = list(Path(input_directory).glob('*.nc'))
    rec = xr.open_dataset(batches.pop())
    wrec = xr.zeros_like(rec) + weight

    while batches:
        print(len(batches))
        das = [xr.open_dataset(batches.pop()) for _ in range(batches_per_iter) if batches]
        ws = [ xr.zeros_like(da) + weight for da in das]
        wdas = [ da * w for da, w in zip(das, ws)]
        rec = outer_add_das([rec, *wdas])
        wrec = outer_add_das([wrec, *ws])

    Path(output_path).parent.mkdir(exist_ok=True, parents=True)
    (rec / wrec).to_netcdf(output_path)

def better_merge_batches(
    input_directory='data/inferred_batches',
    output_path='method_outputs/merged_batches.nc',
    weight=build_weight(patch_dims=dict(time=15, lat=240, lon=240)),
    out_coord_ds='data/prepared/gridded.nc',
    out_shape=xr.open_dataarray('data/prepared/gridded.nc').shape,
    dims_labels=('time', 'lat', 'lon'),
    out_var='ssh',
):

    rec_da = xr.DataArray(
        np.zeros(out_shape), dims=dims_labels, coords=xr.open_dataarray('data/prepared/gridded.nc').coords
    )

    count_da = xr.zeros_like(rec_da)
    batches = list(Path(input_directory).glob('*.nc'))

    for b in tqdm.tqdm(batches):
        da = xr.open_dataarray(b)
        w = xr.zeros_like(da) + weight
        wda = da * w
        coords_labels = set(dims_labels).intersection(da.coords.dims)
        da_co = {c: da[c] for c in coords_labels}
        rec_da.loc[da_co] = rec_da.sel(da_co) + wda
        count_da.loc[da_co] = count_da.sel(da_co) + w

    Path(output_path).parent.mkdir(exist_ok=True, parents=True)
    (rec_da / count_da).to_dataset(name=out_var).to_netcdf(output_path)


#%%
better_merge_batches()

#%%

import xarray as xr
ds = xr.open_dataarray('method_outputs/merged_batches.nc')
ds.close()
ds.isel(time=slice(None, 4)).plot(col='time', col_wrap=2)
#%%
"""## Compute metrics"""
!qf_alongtrack_metrics_from_map \
 stages.interp_on_track.grid_path=method_outputs/merged_batches.nc\
 stages.interp_on_track.grid_var=ssh
# %%
